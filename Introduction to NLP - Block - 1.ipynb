{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Langugage Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "NLP or Natural Language Processing as is normally referred to, refers to working (or processing) **text data**, either for machine learning, or any of the host of use cases textual data comprises of. Working with text, is very different from working with numerical or categorical data. We have worked extensively with data, numerical, categorical and boolean, however text data is a different paradigm altogether and this tutorial aims to get you acquanited with the basics of working with text and understanding the underlying implications in Machine learning.  \n",
    "\n",
    "## Overview\n",
    "\n",
    "- Introduction to the problem statement **Consumer Complaints Database**\n",
    "- What is NLP (Introduction and usecases)\n",
    "- Tokenization and Introduction to NLTK\n",
    "- Vectorization and vector space models **Count Vectorizer**\n",
    "- Applying our first classification algorithm **Logistic Regression**\n",
    "- Stopwords\n",
    "- Basic Stemming \n",
    "- TFIDF\n",
    "- Naive Bayes Classifier\n",
    "- Linear kernel SVM\n",
    "- Text Classification (Build a text classifier using NLTK)\n",
    "\n",
    "\n",
    "## Pre-requisite\n",
    "\n",
    "- Python (along with NumPy and pandas libraries)\n",
    "- Basic statistics (knowledge of central tendancy)\n",
    "\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "- Understanding why working with text data isn't like numerical or categorical data\n",
    "- What is NLP\n",
    "- The basic building blocks of text\n",
    "- Tokenization, Stemming and what constitutes as a stopword\n",
    "- Preliminary cleaning of text data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Introduction to text data\n",
    "\n",
    "### Description: \n",
    "Uptill here, all of our problem statements had data in either a numerical format, a categorical format, or a Boolean format. In the real-world we usually do, and might very well encounter text data. We will now try to understand how we can use text analytics to solve data with text.\n",
    "\n",
    "### 1.1 Introduction to the problem statement: <font color='green'> Categorize complaints into categories</font>\n",
    "\n",
    "**What is the problem?**\n",
    "#### The Dataset is a consumer complaints database where every complaint needs to be categorized into one of the pre-defined 12 categories. A multi- class classification problem. \n",
    "\n",
    "Each row in the dataset describes a single compliant. Including the complaint narrative, the issue, the category of the complaint, the date it was received on, the zip code and details of the customer placing the complaint and the current status of the complaint. The final idea is to build a model that will categorize each customer's complaint into a product (12 categories in all). You can download the dataset below.\n",
    "\n",
    "We will work with the csv file. \n",
    "\n",
    "https://catalog.data.gov/dataset/consumer-complaint-database\n",
    "\n",
    "However, for the purpose of understanding how text processing works, we will specifically, work on only 2 columns of this dataset. It is evident that if we add more features, the model accuracy will rise and be more robust, however initially, we will just have 2 columns - the consumer complaint narrative and the product the complaint has to be categorized into. \n",
    "\n",
    "- Consumer Complaint Narrative\n",
    "- Product\n",
    "\n",
    "The 2 categories the compliants need to be categorized into are: \n",
    "\n",
    "['Mortgage', 'Student loan', 'Credit card or prepaid card', 'Credit card', 'Debt collection', 'Credit reporting', 'Credit reporting, credit repair services, or other personal consumer reports', 'Bank account or service', 'Consumer Loan', 'Money transfers', 'Vehicle loan or lease', 'Money transfer, virtual currency, or money service', 'Checking or savings account', 'Payday loan', 'Payday loan, title loan, or personal loan', 'Other financial service', 'Prepaid card']\n",
    "\n",
    "**Brief explanation of the dataset & features**\n",
    "\n",
    "* `Consumer Complaint Narrative`: Is a paragraph (or text) written by the customer explianing his complaint in detail. It is not a numerical or categorical type, the data is a string type consisting of text in the form of paragraphs\n",
    "    \n",
    "* `Product`: Is the category we are to classify each complaint to.\n",
    " \n",
    "**What we want as the outcome?**\n",
    "\n",
    "Using a classification algorithm, classify each complaint to it's respective category\n",
    "\n",
    "#### Why work with text\n",
    "\n",
    "***\n",
    "\n",
    "**Intuition for text**\n",
    "\n",
    "Let's start with what information we have: The main goal is to build a machine learning model which can predict the category of the complaint based on the customer's written data. The written data here is in the form of a paragraph comprising of sentences (or natural language). How do we convert this text data to a form fit for machine learning? The usual ways of working with numerical or categorical data will not work here, as the data type is completely different, and the algorithm has to make sense of the written data, not a single variable unlike categorical or numerical. \n",
    "\n",
    "\n",
    "If you have a look at the Consumer Complaint Narrative columns, the values are paragraphs! Not numbers or categories. This needs to be pre-processed before running an algorithm onto this. \n",
    "\n",
    "**Why NLP for this data**\n",
    "\n",
    "These complaints in the Narratives columns are typical examples of text data. Normal paragraphs, sentences in the form of text. The column Consumer complaint narrative has all rows in the form of either NaNs or text data. How do we make sense of this data?\n",
    "\n",
    "Do we convert this to categorical by one-hot encoding the text? If yes, how do we do it?\n",
    "\n",
    "How are we supposed to convert this text data to a numerical format to make sure the Machine Learning Algorithm can be applied to this?\n",
    "\n",
    "Can this column be used in a multi category classfication model to predict the class of the complaint?\n",
    "All these questions (and others) can be answered through a particular branch of ML. Enter Natural Language Processing. \n",
    "\n",
    "### Have a look at the data set \n",
    "\n",
    "In this task you will load Consumer_complaints.csv into a dataframe using pandas and explore the column Consumer Complaint Narrative.\n",
    "\n",
    "We will see at the end of this exercise that the **The cell values of the consumer complaint narrative column is a paragraph!** \n",
    "\n",
    "This is a typical example of text data. Normal paragraphs, sentences in the form of text. The column Consumer complaint narrative has all rows in the form of either NaNs or text data. How do we make sense of this data?\n",
    "Do we convert this to categorical by one-hot encoding the text? If yes, how do we do it?\n",
    "How are we supposed to convert this text data to a numerical format to make sure the Machine Learning Algorithm can be applied to this?\n",
    "Can this column be used in a multi-nominal classfication model to predict the class of the complaint?\n",
    "\n",
    "All these questions (and others) can be answered through a particular branch of ML. **Enter Natural Language Processing.**\n",
    "\n",
    "\n",
    "### Instructions\n",
    "- Load the csv into a dataframe\n",
    "- Drop all columns except the Product and the Consumer Complaints Narratives. Make sure to keep a copy of the original dataframe in a different instance. \n",
    "- Print out the first 5 instances of our 2 column dataframe. Name it df. \n",
    "- Rename column Consumer compliant narrative to X and Product to y (The reason we have done this is obvious. We intend to classify the complaints in the narrative (X) to each of the categories listed in the Product (or the Y column))\n",
    "- print out the first value of the X column\n",
    "- Have a look at the various categories that exist in the Product (the renamed y column, these are the categories that exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
      "\n",
      "\n",
      "['Mortgage', 'Student loan', 'Credit card or prepaid card', 'Credit card', 'Debt collection', 'Credit reporting', 'Credit reporting, credit repair services, or other personal consumer reports', 'Bank account or service', 'Consumer Loan', 'Money transfers', 'Vehicle loan or lease', 'Money transfer, virtual currency, or money service', 'Checking or savings account', 'Payday loan', 'Payday loan, title loan, or personal loan', 'Other financial service']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"Consumer_complaints.pkl\")\n",
    "df_copy = df.head(1000).copy()\n",
    "df = df_copy[[\"Consumer complaint narrative\", \"Product\"]] #keeping the relevant columns\n",
    "df.columns = [\"X\",\"y\"]\n",
    "df.head()\n",
    "#Printing out the first non-empty value of the X column. Hence the second value, index is 1\n",
    "print(df[\"X\"][1]) \n",
    "print (\"\\n\")\n",
    "print (list(df[\"y\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: What is NLP? (Introduction and Usecases)\n",
    "\n",
    "### 2.1 Introduction\n",
    "\n",
    "***\n",
    "\n",
    "NLP or the Natural Language part in NLP, is called that, because it is the language that exists all around us. \n",
    "NLP can broadly be defined as the \"cleaning\" and \"getting the text\" to a form fit for machine learning. That's all it really is. Of course you can derive insights from the text as well just like the EDA operation, which aims to bring the data to a more ML application frinedly approach. There are other off shoots of Natural Langugage such as NLG - Natural language Generation which aims to generate new text data based on prior data and Natural Language Understanding - NLU which is the backbone of all intelligent chatbots out there currently, which focuses on recognizing the intent of a conversation. For the sake of this tutorial and brevity we will stick to NLP. \n",
    "\n",
    "#### Why is it difficult to work with text?\n",
    "\n",
    "***\n",
    "\n",
    "Comprehending Language is hard for computers. Several reasons exist. \n",
    "\n",
    "- Different ways of saying the same thing, is one of the prime reasons, why computers have a hard time deciphering the meaning or intent of those statements. \"I like the rains of Mumbai\" and \"Mumbai is beautiful during the monsoons and that's why I like it\" are basically advocating the same sentiment, however since they are 2 completely different sentences syntactically, computers have a hard time figuring out the intent of the user and get stuck.\n",
    "\n",
    "- Ambiguity - \"The shop is by the road\" and \"The shop was found to be closed by him\". Are 2 completely different statements. The word \"by\" used in completely different meanings here. In the first case it represents proximity, in the second it refers to the person. \n",
    "\n",
    "- Context. \"Anil is my friend. He likes football\". In the second statement \"he\" refers to Anil. Computers are not inherently able to store the context of the first statement and use it to the decipher the second statement. \n",
    "\n",
    "- Understanding language. All code to a machine is just numbers. A statement in human language is just a sequence of numbers to a computer. Rudimentary Chatbots work because they detect key words in your statement. As long as the keywords remain the same, you could use any words in your statements and the end result will remain the same. \n",
    "\n",
    "- Every language has its own uniqueness. Like in the case of English we have words, sentences, paragraphs and so on to limit our language. But in Thai, there is no concept of sentences. That’s why Google Translator or any other translator struggles to perfectly convert a piece of text from one language to another.\n",
    "\n",
    "- Machines have a hard time adapting to any new constructs that humans come up with. Suppose a teenager is looking at his twitter feed and comes across a word he has never seen before, he might not understand it’s meaning instantly. But this does not mean he cannot adapt. After looking at the word in several different tweets he might be able to understand why and in which context that word is to be used. This is not possible with machines. Machines can only handle data that they have seen before. If something new comes up, they get confused and are unable to respond.\n",
    "\n",
    "#### Usecases of NLP\n",
    "\n",
    "***\n",
    "\n",
    "The usecases of NLP encompass almost anything you can do with Language in relation to a problem. \n",
    "\n",
    "1) Sentiment Analysis - Finding if the text opinionated a positive or negative sentiment.\n",
    "\n",
    "(Sentiment analysis is immensely useful in figuring the overall sentiment of products (Amazon), movies (Netflix), food (Zomato) by parsing the reviews and doing a sentiment analysis on them)\n",
    "\n",
    "2) Text Classfication - categorizing text to various categories\n",
    "(Some examples of text classification are:\n",
    "\n",
    "- Understanding audience sentiment from social media,\n",
    "- Detection of spam and non-spam emails,\n",
    "- Auto tagging of customer queries, and\n",
    "- Categorization of news articles into defined topics.\n",
    ")\n",
    "\n",
    "3) Summarizing - Summarzing a paragraph into \"n\" words or sentences\n",
    "\n",
    "(Example: Inshorts, news in 60 words or less)\n",
    "\n",
    "4) Parts of Speech - Tagging - Figuring out the various nouns, adverbs, verbs etc; in your text\n",
    "\n",
    "(Chatbots)\n",
    "\n",
    "5) Language translation\n",
    "\n",
    "(Google translate)\n",
    "\n",
    "6) Grammar correction\n",
    "\n",
    "(Autocorrect in messaging services)\n",
    "\n",
    "7) Entity recognition - Finding places, animals, people from the text in question\n",
    "\n",
    "(Chatbots\n",
    "\n",
    "8) Intent recognition - Chatbots usually use this extensively. To figure what exactly you, or the customer in question needs information or services about. \n",
    "\n",
    "We will deal with each of them in detail in the subsequent tutorials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Tokenization and Introduction to NLTK\n",
    "\n",
    "\n",
    "### 3.1 Building blocks of text:Motivation for tokenization\n",
    "\n",
    "***\n",
    "Now we can see that unlike all the machine learning datasets we have worked with previously, the data isn't boolean, numeric, cateorigical etc; How do we apply this text data to a ML algorithm?\n",
    "The first step is understanding what text data consists of..\n",
    "\n",
    "Usually a text is composed of paragraphs, paragraphs are composed of sentences, and sentences are composed of words. \n",
    "\n",
    "#### Words are the basic building blocks of any text. \n",
    "\n",
    "Sure you could do deeper into letters, but the letters as themselves have no meaning, it's only when they are combined into words, that the text starts to make sense. Hence NLP considers words as the absolute unit of text. \n",
    "\n",
    "Tokenization is exactly what it sounds like. Breaking down anything to \"tokens\". Tokens are the basic units of a particular dataset. In this case, our data is text and tokenization implies breaking it down into it's basic tokens. Which are words. \n",
    "\n",
    "We could also tokenize a paragrah into sentences. Since a paragraph is composed of sentences. \n",
    "\n",
    "#### Introduction to NLTK\n",
    "\n",
    "***\n",
    "Working with text data as we have seen is not as straightforward as working with numeric or other data types, hence it is no surprise, that this processing is achieved through special libraries. NLTK or the Natural Langugage Tool Kit is the de-facto standard library in python which specifically deals with text. Tasks, such as tokenization, Lemmatization, text_classification, vectorizing are methods built in and help working with text much simpler. Do not worry if the above terms do not make sense to you, we will get to them and cover them in detail eventually. \n",
    "\n",
    "NLTK is not part of the standard Anaconda 3.6 installation and will require an independent set-up.\n",
    "\n",
    "#### Installing NLTK. \n",
    "\n",
    "***\n",
    "To install NLTK and all it's dependencies, go to your terminal(mac) or command window(windows) and type pip install nltk. This could take a while, depending on your internet speed. It is to be noted that NLTK is not the only library that can be used for text processing, there are a ton of others, however NLTK is one of the first ones that came up and is generally astarting point from a beginner's perspective. There are obvious tasks the NLTK library cannot do, which have to be compensated via other libraries, howeer for thr purpose of this particukar tutorial, NLTK should more than suffice presently. \n",
    "\n",
    "#### Tokenizing with NLTK - The problem intuition \n",
    "\n",
    "***\n",
    "\n",
    "Now that we have already defined our present dataframe df as below, we will need to find a way to convert the text in the X column to numbers to get them to a form where I would be able to apply an alogorithm to this. Think of this like sklearn, which require all non-numeric data to be encoded (label or one-hot) prior to the sklearn pipeline. \n",
    "\n",
    "Intuitively, it would make sense to divide each paragraph of text to it's basic form (words) and then convert each of those words to numbers. We could assign a particular number to each word, in which case a sentence could look like a set of numbers to us, each number representing a particular word. \n",
    "\n",
    "The first step to acheiving that would be to break the text down to words. That's what tokenization aims to do. NLTK has a built in for tokenization. Assuming NLTK is installed on your machines now, let us quickly run through a few sample tokenization exercises, before your assignment 3 where you will break doen every cel in the Y column to its words and create a new colunm for the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the first complaint into words\n",
    "\n",
    "In this task you will assign a variable to the first row of the consumer complaints narrative column (X column) and break down the text into it's constituent words.\n",
    "### Instructions\n",
    "- Load the dataframe defined earlier\n",
    "- Assign a variable **first_complaint** to the paragraph listed in the first non-empty row of the consumer complaint narrative column (or X)\n",
    "- Break it down into words using the split command initially and then using the nltk.word_tokenize function. \n",
    "- Assign this list of words to another list called bag_of_words\n",
    "- We can see from both lists (the one using split and the one using word_tokenize) that the word_tokenize function is more robust as it splits the paragraph into purely words and seperates the punctuation into seperate tokens. In the case of split, full-stops have appeared along with certain words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
      "\n",
      "\n",
      "Using the Split Command\n",
      "\n",
      "\n",
      "['When', 'my', 'loan', 'was', 'switched', 'over', 'to', 'Navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'XXXX', 'i', 'did', 'not.', 'When', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'XXXX', 'into', 'the', 'XXXX.', 'I', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan.', 'I', 'was', 'told', 'that', 'Navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with.', 'I', 'contacted', 'Navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me.', 'I', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus.', 'I', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up.']\n",
      "\n",
      "\n",
      "['When', 'my', 'loan', 'was', 'switched', 'over', 'to', 'Navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'XXXX', 'i', 'did', 'not', '.', 'When', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'XXXX', 'into', 'the', 'XXXX', '.', 'I', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan', '.', 'I', 'was', 'told', 'that', 'Navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with', '.', 'I', 'contacted', 'Navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me', '.', 'I', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus', '.', 'I', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ashwani.saxena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "df[\"X\"].head(3)\n",
    "df = df.dropna() #dropping nans\n",
    "first_complaint = df[\"X\"].iloc[0]\n",
    "print (first_complaint)\n",
    "print (\"\\n\")\n",
    "#Using the split command\n",
    "print (\"Using the Split Command\")\n",
    "print (\"\\n\")\n",
    "bag_of_words = first_complaint.split(\" \")\n",
    "print (bag_of_words)\n",
    "\n",
    "bag_of_words = word_tokenize(first_complaint)\n",
    "print (\"\\n\")\n",
    "print (bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sent Tokenize\n",
    "\n",
    "### Tokenize the first complaint into  sentences\n",
    "\n",
    "One could also tokenize a paragraph into constituent sentences. \n",
    "\n",
    "### Assignment 2\n",
    "\n",
    "#### Tokenize the Second non-empty complaint into words and convert all words to lower case and assign the list of words to a list\n",
    "\n",
    "#### Description:\n",
    "The importance of converting words to lower case - All words should be converted to lowercase while doing NLP. The reason behind being, that \"Mumbai\" and \"mumbai\" even though are the same word, will be considered 2 seperate words whilst converting the words into numbers, and that would be a biased conversion. To avoid such problems, it is standard practice to convert all words or text to lower case, before beginning NLP. \n",
    "\n",
    "### Instructions\n",
    "- Load the dataframe defined earlier\n",
    "- Assign a variable **first_complaint** to the paragraph listed in the first non-empty row of the consumer complaint narrative column (or X)\n",
    "- Break it down into sentences using the sent_tokenize function from nltk\n",
    "- Assign this list of words to another list called list_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I tried to sign up for a spending monitoring program and Capital One will not let me access my account through them\n",
      "\n",
      "\n",
      "['i', 'tried', 'to', 'sign', 'up', 'for', 'a', 'spending', 'monitoring', 'program', 'and', 'capital', 'one', 'will', 'not', 'let', 'me', 'access', 'my', 'account', 'through', 'them']\n"
     ]
    }
   ],
   "source": [
    "first_complaint #Is already loaded onto the workspace\n",
    "from nltk.tokenize import sent_tokenize\n",
    "list_of_sentences = sent_tokenize(first_complaint)\n",
    "(list_of_sentences)\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "print (df[\"X\"].iloc[1])\n",
    "bag_of_words_lower = word_tokenize(df[\"X\"].iloc[1].lower())\n",
    "print (\"\\n\")\n",
    "print (bag_of_words_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Vectorization\n",
    "\n",
    "\n",
    "### 4.1 Converting your text to numbers: The crux of NLP\n",
    "\n",
    "***\n",
    "We managed to convert our complaints into a bag of words or a list of words. But that is no good until we figure a way out to convert these words to a numeric format. And that is necessary to apply any sort of algorithm (machine learning or otherwise). \n",
    "\n",
    "**This process of converting text data to numbers is called vectorization**\n",
    "\n",
    "There are multiple methods to convert words to numbers. We will be initially dealing with 1 of them: A count Vectorizer. \n",
    "\n",
    "### 4.2 The intuition behind vectorization: The count vectorizer\n",
    "\n",
    "***\n",
    "Every list of words corresponds to a row in our dataframe, something like this. Where the X column would now be the list of lowercased words and our y column would be the product category. \n",
    "\n",
    "<img src=\"img.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea now is to convert the X column to numbers. \n",
    "\n",
    "**One way to do that would be to represent every word as a key value pair in the form of a dictionary, where the key would be the word and the vale would be the number of times that word has appeared in the list.** \n",
    "\n",
    "This method of converting the counts of words in the list to convert them to a numeric format is called Count vectorization. We will initially do this manually, and then explot sklearn to do this automatically to understand the intuition behind this. \n",
    "\n",
    "\n",
    "### Convert the first complaint to numbers using the counts of words in the form of a dictionary\n",
    "\n",
    "In this task you will implement your own code for count vectorization\n",
    "### Instructions\n",
    "- Load the df.\n",
    "- Take the first complaint in the X column and assign that to a list called **first_complaint** \n",
    "- Tokenize the list to its words and convert them to lower case. \n",
    "- Create a dictionary (any method you prefer) so that the keys are the words themselves and the values are the number of times the word has appeared in the list **first_complaint**\n",
    "- Name this dictionary as Count_Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 11, 'the': 8, '.': 7, 'was': 5, 'to': 5, 'and': 5, 'my': 4, 'had': 4, 'credit': 4, 'navient': 3, 'told': 3, 'that': 3, 'with': 3, 'xxxx': 3, 'when': 2, 'loan': 2, 'a': 2, 'balance': 2, 'score': 2, 'been': 2, 'have': 2, 'delinquency': 2, 'just': 2, 'bureaus': 2, 'so': 2, 'switched': 1, 'over': 1, 'never': 1, 'deliquint': 1, 'because': 1, 'did': 1, 'not': 1, 'going': 1, 'purchase': 1, 'vehicle': 1, 'discovered': 1, 'dropped': 1, 'from': 1, 'into': 1, 'faithful': 1, 'at': 1, 'paying': 1, 'student': 1, 'company': 1, 'contacted': 1, 'resolve': 1, 'this': 1, 'issue': 1, 'you': 1, 'kept': 1, 'being': 1, 'contact': 1, 'expalin': 1, 'situation': 1, 'maybe': 1, 'they': 1, 'could': 1, 'help': 1, 'me': 1, 'angry': 1, 'hurried': 1, 'paid': 1, 'off': 1, 'then': 1, 'after': 1, 'tried': 1, 'dispute': 1, 'much': 1, 'trouble': 1, 'bringing': 1, 'back': 1, 'up': 1})\n"
     ]
    }
   ],
   "source": [
    "first_complaint = word_tokenize(df[\"X\"].iloc[0].lower())\n",
    "from collections import Counter\n",
    "Count_Vectorizer = {}\n",
    "Count_Vectorizer = Counter(first_complaint)\n",
    "print (Count_Vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Introduction to the sklearn's Count Vectorizer\n",
    "\n",
    "***\n",
    "We did manage to convert our list of words to numbers. However the problem still remain unresolved. \n",
    "\n",
    "**How do we apply our algorithm to this?**\n",
    "\n",
    "**Could we convert every word to a feature (or column) and the count associated with it to it's value and then apply a Classification algorith to it? Something like below?\n",
    "\n",
    "<img src=\"img2.png\">\n",
    "\n",
    "** This looks very similar to one-hot encoding and is a typical method of applying ML to text data**. \n",
    "\n",
    "This is simiar to one-hot in the way that when we add the second row, and the the third row and subsequent rows, the features or the columns will increase as more and more words come in and there will be words which do not appear in say the first_complaint, the vectorizer will automatically assign 0 to those words. **Hence the number of features will be equal to the total number of unique words in all the complaints combined and the values for those features will be the count of those words in that particular complaint.**\n",
    "\n",
    "A normal classifcation algorithm can now be applied where X is all the features except the Product column and y is the Product column. \n",
    "\n",
    "We could just use the sklearn's Count Vectorizer and convert all the text into numbers in a single step instead of breaking them down into individual words, lowercasing them, and then making a dictionary assigning the counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we would do it for the first row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When my loan was switched over to Navient i wa...</td>\n",
       "      <td>Student loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I tried to sign up for a spending monitoring p...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My mortgage is with BB &amp; T Bank, recently I ha...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The entire lending experience with Citizens Ba...</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>My credit score has gone down XXXX points in t...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    X  \\\n",
       "1   When my loan was switched over to Navient i wa...   \n",
       "2   I tried to sign up for a spending monitoring p...   \n",
       "7   My mortgage is with BB & T Bank, recently I ha...   \n",
       "13  The entire lending experience with Citizens Ba...   \n",
       "14  My credit score has gone down XXXX points in t...   \n",
       "\n",
       "                              y  \n",
       "1                  Student loan  \n",
       "2   Credit card or prepaid card  \n",
       "7                      Mortgage  \n",
       "13                     Mortgage  \n",
       "14             Credit reporting  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.']\n",
      "\n",
      "\n",
      "Applying the count vectorizer\n",
      "Vector Shape\n",
      "\n",
      "\n",
      "(1, 69)\n",
      "\n",
      "\n",
      "vector Values\n",
      "[[1 5 1 1 1 2 1 2 1 1 2 1 1 1 1 4 2 1 1 1 1 1 1 1 1 1 4 2 1 1 1 1 2 1 2 1\n",
      "  1 1 4 3 1 1 1 1 1 1 1 1 2 1 2 1 1 3 8 1 1 1 5 3 1 1 1 1 5 2 3 3 1]]\n",
      "These are the counts of the 69 unique words in our first complaint. To find which words have these counts, we can execute the command below:cv.vocabulary_\n",
      "\n",
      "\n",
      "Count Vectorizer Vocabulary\n",
      "{'when': 65, 'my': 38, 'loan': 34, 'was': 64, 'switched': 52, 'over': 43, 'to': 58, 'navient': 39, 'never': 40, 'told': 59, 'that': 53, 'had': 26, 'deliquint': 17, 'balance': 5, 'because': 6, 'with': 66, 'xxxx': 67, 'did': 18, 'not': 41, 'going': 25, 'purchase': 46, 'vehicle': 63, 'discovered': 19, 'credit': 15, 'score': 48, 'been': 7, 'dropped': 21, 'from': 24, 'the': 54, 'into': 30, 'have': 27, 'faithful': 23, 'at': 3, 'paying': 45, 'student': 51, 'company': 11, 'delinquency': 16, 'contacted': 13, 'resolve': 47, 'this': 57, 'issue': 31, 'you': 68, 'and': 1, 'kept': 33, 'being': 8, 'just': 32, 'contact': 12, 'bureaus': 10, 'expalin': 22, 'situation': 49, 'maybe': 35, 'they': 56, 'could': 14, 'help': 28, 'me': 36, 'so': 50, 'angry': 2, 'hurried': 29, 'paid': 44, 'off': 42, 'then': 55, 'after': 0, 'tried': 60, 'dispute': 20, 'much': 37, 'trouble': 61, 'bringing': 9, 'back': 4, 'up': 62}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "txt = [df[\"X\"].iloc[0]]\n",
    "print (txt)\n",
    "print (\"\\n\")\n",
    "print (\"Applying the count vectorizer\")\n",
    "cv.fit(txt)\n",
    "vector = cv.transform(txt)\n",
    "print (\"Vector Shape\")\n",
    "print (\"\\n\")\n",
    "print (vector.shape)#Has 69 unique words\n",
    "vector_values = vector.toarray()\n",
    "print (\"\\n\")\n",
    "print (\"vector Values\")\n",
    "print (vector_values)\n",
    "print ('These are the counts of the 69 unique words in our first complaint. To find which words have these counts, we can execute the command below:cv.vocabulary_')\n",
    "print (\"\\n\")\n",
    "print (\"Count Vectorizer Vocabulary\")\n",
    "print (cv.vocabulary_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This does not specify the counts of the word. This specifies the index of the word in the vector_values list. So for the first word expalin, we see it's index is 22, if we see the index 22 in the vector_values list, we will see that it's value is 1,as specified in the figure above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 3, 8, 1, 1, 1, 5, 3, 1, 1, 1, 1, 5, 2, 3, 3, 1]\n",
      "\n",
      "\n",
      "count value of the word at index 22\n",
      "1\n",
      "\n",
      "\n",
      "count value of the word at index 34, the word is 'loan'\n",
      "2\n",
      "\n",
      "\n",
      "Seeing the cv.vocabulary_ dictionary we see that the word is 'loan' and it's value in the vector values list is 2. \n"
     ]
    }
   ],
   "source": [
    "vector_values = vector_values.tolist()\n",
    "vector_values = vector_values[0]\n",
    "print (vector_values)\n",
    "print (\"\\n\")\n",
    "print (\"count value of the word at index 22\")\n",
    "print (vector_values[22]) #Value of 1\n",
    "print (\"\\n\")\n",
    "print (\"count value of the word at index 34, the word is 'loan'\")\n",
    "print (vector_values[34]) #Value of 2\n",
    "print (\"\\n\")\n",
    "print (\"Seeing the cv.vocabulary_ dictionary we see that the word is 'loan' and it's value in the vector values list is 2. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the count vectorizer and getting the data to a form for algorithm application. \n",
    "\n",
    "#### Use the count vectorizer to numerize the X column of the dataframe and make a new dataframe with these features and the product column. We will consider just the top 3 rows of the entire dataframe to aid better understanding and then run it over the entire dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n",
      "\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "#Importing count vectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Initializing the Count vectorizer\n",
    "cv = CountVectorizer()\n",
    "#Initializing a dataframe \"all text\" with the first 3 rows of df\n",
    "all_text = df[\"X\"][:3]\n",
    "all_text = pd.DataFrame(all_text)\n",
    "#Renaming the column for that dataframe (has only one column) to \"text\"\n",
    "all_text.columns = [\"Text\"]\n",
    "#Converting to lower case\n",
    "all_text[\"Text\"] = all_text['Text'].str.lower()\n",
    "all_text\n",
    "#Fitting the Count vectorizer all text\n",
    "cv.fit(all_text[\"Text\"])\n",
    "vector = cv.transform(all_text[\"Text\"])\n",
    "vector_values_array = vector.toarray()\n",
    "#Converting the text to numbers - The transform function does this. \n",
    "vector_values_list = vector_values_array.tolist()\n",
    "print (len(vector_values_list)) \n",
    "print (\"\\n\")\n",
    "#Because there are 3 rows in the entire dataframe. \n",
    "print (len(vector_values_list[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "214 is the number of unique words in all 3 rows combined. This value will be constant for every list element in the vector_values_list because all the unique words in the entire dataframe have been converted to features and the values for these features per row depends on the count of those words in that row, 0 in case the word does not exist in the row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_values_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_values_list[2]) \n",
    "\n",
    "#As you can see that every document has been converted to a fixed length vector of 214 words, and \n",
    "#have values coresponding to the occurences of those words in the particular document:0 in case those words\n",
    "# words aren't present in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The y values for these 3 rows are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to label encode these categories to numerize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   labels\n",
      "1       2\n",
      "2       0\n",
      "7       1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labels = pd.DataFrame(df[\"y\"][:3])\n",
    "#label encoding the y values\n",
    "labels.columns = [\"labels\"]\n",
    "le = LabelEncoder()\n",
    "labels[\"labels\"] = le.fit_transform(labels[\"labels\"])\n",
    "print (labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our final dataframe for the X is the vector_values_array, which is the vectorized form of the text, and the Y is the labels dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 1, 0, 0, 0, 1, 1, 2, 0, 0,\n",
       "        0, 0, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 4, 0, 0, 0, 2, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 4, 2, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 4, 3, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 3, 8,\n",
       "        0, 0, 1, 1, 0, 1, 0, 5, 3, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 5, 0,\n",
       "        0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 3, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_values_array[:1][:5] #This is the row 1 of the original dataframe df now numerized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now, include all the 335 rows and vectorize them and label it as the X, and the corresponding y labels, we can train a classification algorithm on the same, and figure out the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the exact same code as the earlier one on 335 rows\n",
    "import numpy as np\n",
    "all_text = df[\"X\"]\n",
    "all_text = pd.DataFrame(all_text)\n",
    "all_text.columns = [\"Text\"]\n",
    "all_text[\"Text\"] = all_text['Text'].str.lower()\n",
    "cv = CountVectorizer()\n",
    "cv.fit(all_text[\"Text\"])\n",
    "vector = cv.transform(all_text[\"Text\"])\n",
    "vector_values_array = vector.toarray()\n",
    "labels = pd.DataFrame(df[\"y\"])\n",
    "labels.columns = [\"labels\"]\n",
    "labels[\"labels\"] = le.fit_transform(labels[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_values_array) #Because 335 documents in all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels) #Because each document or complaint has a label, hence 335 labels in all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a normal Logistic Regression function to this X and y post breaking it down to a train and a test set, we can calculate the accuracy of the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "X = vector_values_array\n",
    "y = labels[\"labels\"]\n",
    "X_train,X_test,y_train,y_test = tts(X,y,test_size=0.4,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4235294117647059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwani.saxena\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ashwani.saxena\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "log_reg.fit(X_train,y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print (accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a 42% accuracy of predicting the product category. Which isn't really a good number. A look at the precision recall shows that the product category was heavily imblanaced, hence a few categories have not been detected by the algorithm. Before we rectify the same, let us first look at the distribtion of the y column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     69\n",
       "7     52\n",
       "5     17\n",
       "9     16\n",
       "4     12\n",
       "1     11\n",
       "11    10\n",
       "3      6\n",
       "2      5\n",
       "0      5\n",
       "8      4\n",
       "12     2\n",
       "10     2\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[\"labels\"].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see very clearly that the data set clearly has heavily imbalanced labels. Labels 10,8,12 are under represented and hence the probability of the model catching those labels is less, hence the accuracy is going to suffer. A look at the classification report should reinfornce this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.50      0.50      0.50         4\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.20      0.33      0.25         3\n",
      "           5       0.67      0.25      0.36         8\n",
      "           6       0.42      0.65      0.51        23\n",
      "           7       0.46      0.62      0.53        21\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       0.33      0.43      0.38         7\n",
      "          10       0.00      0.00      0.00         1\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.42      0.42      0.42        85\n",
      "   macro avg       0.20      0.21      0.19        85\n",
      "weighted avg       0.35      0.42      0.37        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print (classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted, the recall for Labels, 8,10,11,12,2,0 and 3 are 0. A typical sampling issue here. This can be rectified by an oversampling technique such as SMOTE or ROS, however before we get to that, it's time to understand what the other reasons for a lower accuracy could be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to realize that count vectorizer is essentially a Ranking algorithm, in the way that it gives a higher weight to words which have appeared more number of times, in other words, the value of the key_wvalue pair is plain the count of the word in the dataset. It does not assign any importance to the order in whoch the words are sentences have appeared. \n",
    "\n",
    "Another point to be considered is the fact that words like \"a\",\"an\",\"the\"... will appear more number of times than the rest of the words as they are common articles. Using a Count vectorizer out of the box on a paragraph or a body of text will invariably give the highest count to these common words. Hence the words we are actually interested in will be underneath these words. one way to rectify this, is to remove these commonly occurng words. NLTK offers this functionality and has rightly defined these particluar words as \"stopwords\" or words we wouldn't include in out bag of words. We will have to remove the punctutaion as well, as you can see our initial bag of words had the commas, full-stops and all of the other symbols as well. We will want to remove them as well, to avoid being vectorized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the list of stopwords, NLTK currently includes, we could check that by just running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'when', 'against', \"shan't\", 'be', 'my', 'all', \"weren't\", 'doing', 'very', 'for', \"mustn't\", 'their', 'it', 'other', 'can', 'again', 'him', 'been', 'herself', 'both', 'wasn', 'isn', \"don't\", \"you'd\", 'few', 'we', 'theirs', 'own', 'his', 'until', 'doesn', 'in', 'weren', 'by', \"hasn't\", 'during', 'who', 'to', 'being', 'over', 'than', 'should', \"wouldn't\", 'each', \"didn't\", 'of', \"shouldn't\", 'hasn', 'or', 'they', 'how', \"hadn't\", 'd', 'through', 'her', 'such', 'have', 'shouldn', 'wouldn', 'yourselves', 'only', 'after', 'hadn', 'before', 'up', 'hers', 'its', 'between', 're', \"she's\", 'once', 'won', 'was', 'ain', 'above', 'there', \"doesn't\", 'from', 'which', 'aren', 'were', 'themselves', 'into', 'down', 'are', \"aren't\", 'what', 'under', 'same', 'with', 'and', 'this', 'where', 'y', \"mightn't\", \"couldn't\", 'ourselves', 'will', 'out', 'did', 'at', \"you're\", 'had', \"you've\", 'ma', \"won't\", 'couldn', 'you', 'your', 'itself', 'about', 's', 't', 'didn', 'those', 'she', 'yours', \"that'll\", 'most', 'he', 'but', 'nor', 'an', 'me', \"you'll\", 'that', 'below', 'any', 'shan', \"wasn't\", 'on', 'am', 'just', 'll', 've', 'here', 'too', 'not', 'whom', 'ours', 'more', 'them', 'our', 'some', 'then', 'as', 'a', 'why', 'has', 'is', 'yourself', 'does', 'myself', 'no', 'o', 'because', 'mustn', 'if', \"should've\", \"isn't\", 'these', 'now', 'further', 'so', \"needn't\", 'himself', \"haven't\", 'mightn', 'haven', 'the', 'do', 'needn', 'm', 'don', 'i', 'having', 'while', \"it's\", 'off'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashwani.saxena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "print (set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The punctutaion list can be derived as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "print (list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#The good thing is we can add our own list of words we want to remove our body of text over and above #this list. \n",
    "\n",
    "custom_set_of_stopwords = set(stopwords.words('english')+list(punctuation)+[\"Bangalore\"])\n",
    "\n",
    "#This will include the word Bangalore as a topword and remoe the same from our body of text before vectorizing it. \n",
    "print (\"Bangalore\" in custom_set_of_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we re-do the exercise for prediction removing the stopwords, the accuracy should increase, as now we are not giving any weight to meaningless words but to words which actually matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = df[\"X\"]\n",
    "all_text = pd.DataFrame(all_text)\n",
    "all_text.columns = [\"Text\"]\n",
    "all_text[\"Text\"] = all_text['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking just the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_complaint = all_text['Text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when my loan was switched over to navient i was never told that i had a deliquint balance because with xxxx i did not. when going to purchase a vehicle i discovered my credit score had been dropped from the xxxx into the xxxx. i have been faithful at paying my student loan. i was told that navient was the company i had delinquency with. i contacted navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. i was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. i have had so much trouble bringing my credit score back up.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_complaint_bow = word_tokenize(first_complaint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'my', 'loan', 'was', 'switched', 'over', 'to', 'navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'xxxx', 'i', 'did', 'not', '.', 'when', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'xxxx', 'into', 'the', 'xxxx', '.', 'i', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan', '.', 'i', 'was', 'told', 'that', 'navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with', '.', 'i', 'contacted', 'navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me', '.', 'i', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus', '.', 'i', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up', '.']\n"
     ]
    }
   ],
   "source": [
    "print (first_complaint_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_complaint_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_complaint_bow_stopwords_removed = [x for x in first_complaint_bow if x not in custom_set_of_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loan', 'switched', 'navient', 'never', 'told', 'deliquint', 'balance', 'xxxx', 'going', 'purchase', 'vehicle', 'discovered', 'credit', 'score', 'dropped', 'xxxx', 'xxxx', 'faithful', 'paying', 'student', 'loan', 'told', 'navient', 'company', 'delinquency', 'contacted', 'navient', 'resolve', 'issue', 'kept', 'told', 'contact', 'credit', 'bureaus', 'expalin', 'situation', 'maybe', 'could', 'help', 'angry', 'hurried', 'paid', 'balance', 'tried', 'dispute', 'delinquency', 'credit', 'bureaus', 'much', 'trouble', 'bringing', 'credit', 'score', 'back']\n"
     ]
    }
   ],
   "source": [
    "print (first_complaint_bow_stopwords_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_complaint_bow_stopwords_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the less important words such as \"a\",\"an\",\"the\",\"where\" have been removed including the punctutaion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redoing this for all the rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = df[\"X\"]\n",
    "all_text = pd.DataFrame(all_text)\n",
    "all_text.columns = [\"Text\"]\n",
    "all_text[\"Text\"] = all_text['Text'].str.lower()\n",
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "cv.fit(all_text[\"Text\"])\n",
    "vector = cv.transform(all_text[\"Text\"])\n",
    "vector_values_array = vector.toarray()\n",
    "labels = pd.DataFrame(df[\"y\"])\n",
    "labels.columns = [\"labels\"]\n",
    "labels[\"labels\"] = le.fit_transform(labels[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "X = vector_values_array\n",
    "y = labels[\"labels\"]\n",
    "X_train,X_test,y_train,y_test = tts(X,y,test_size=0.4,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47058823529411764\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train,y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print (accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a massive 5% increase in accuracy by just removing the stopwords. The model is now being trained on the words which actually matter. Not the more commonly occuring stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Introduction to the TF-IDF Vectorizer\n",
    "\n",
    "In the last tutorial we saw how text was converted to numerics using a count vectorizer. \n",
    "\n",
    "In other words, a count vectorizer, counts the occurences of the words in a document and all the documents are considered independent of each other. Very similar to a one hot encoding or pandas getdummies function. However in cases where multiple documents are involved, count vectorizer still does not assume any interdependence between the documents and considers each of the documents as a seperate entity. \n",
    "\n",
    "It does not rank the words based on their importance in the document, but just based on whether they exist or not. This is not a wrong approach, but it intuitively makes more sense to rank words based on their importance in the document right? In fact, the process of converting, text to numbers should essentially be a ranking system of the words so that the documents can each get a score based on what words they contain. All words cannot have the same imprtance or relevance in the document right?\n",
    "\n",
    "#### Enter TF-IDF!!\n",
    "\n",
    "TF-IDF or Term Frequency and Inverse Document Frequency is kind of the holy grail of ranking metrics to convert text to numbers. Consider the count vectorizer as a metric which just counts the occurences of words in a document. \n",
    "\n",
    "** The ranking system in a count vectorizer is purely occurence based on a single document only!**\n",
    "\n",
    "TF-IDF takes it a step further and ranks the words based not just on their occurences in one document but across all the documents. Hence if CV or Count vectorizer was giving more importance to words because they have appeared multiple times in the document, TF-IDF will rank them high if they have appeared only in that document, meaning that they are rare, hence higher importance and lower if they have appeared in all or most documents, because they are more common, hence lower ranking. \n",
    "\n",
    "Consider a scenario where there are 5 documents and all are talking aout football. The word football would have appeared multiple times in each document. CV is going to rank football consistently high and infact give the word football a different value across all 5 documents based on how many times that word has appeared in that document. In other words, it is assuming, that the more number of times a word appears, the more important it is. That is exactly what the TF or the Term Frequency component in TF-IDF does. \n",
    "\n",
    "IDF on the other hand now is the dominating factor in TFIDF which is going to find out the number of times football has also appeared in the other 4 documents except for the one it is currently seeing. If football has also appeared in rest of the documents, it means that though football is important to that one document based on the number of occurences, considering it has appeared in the rest as well, it is not that rare or more common, hence the importance now is going to reduce instead of going high!\n",
    "\n",
    "**The ranking system is across the entire corpus or all documents.  It is not a single document based metric!**\n",
    "\n",
    "We have seen how CV is calculated for a word in a document. Let us now see how TF IDF is...\n",
    "\n",
    "The tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "#### Example\n",
    "\n",
    "Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
    "\n",
    "Let us now take the first 2 complaints and run a TF-IDF vectorizer on the same. So, in this case, the 2 complaints are our 2 documents. and instead of a CV which considers each document independent of each other and just calculates the count of every word in the document. now the corpus will be the sum total of both documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint_1 = df[\"X\"].iloc[0]\n",
    "complaint_2 = df[\"X\"].iloc[1]\n",
    "complaint_3 = df[\"X\"].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint 1:  When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
      "\n",
      "\n",
      "Complaint 2:  I tried to sign up for a spending monitoring program and Capital One will not let me access my account through them\n",
      "\n",
      "\n",
      "Complaint 3:  My mortgage is with BB & T Bank, recently I have been investigating ways to pay down my mortgage faster and I came across Biweekly Mortgage Calculator on BB & T 's website. It's a nice, easy to use calculator that you plug in your interest rate, mortgage amount, mortgage term, and payment type and it calculates your accelerated bi-weekly payment for you and shows you how much quicker you can pay down your loan. Ours figured out to pay off a 30 year mortgage in 26.4 years ... quite a savings! \n",
      "I called BB & T 's customer service number to inquire how I get set up on this payment plan. I was told they do not offer that type of payment plan, but I could send in my payments bi-weekly but it would not be applied until the full amount was received. ( the money would sit in a \" holding account '' until the full payment amount was collected ). I ended up calling back a few days later thinking the rep I was talking to didn't understand what I wanted to do or was not knowledgeable of this program. I got the SAME ANSWER! \n",
      "I then asked for the corporate BB & T office number where I could speak to someone that was knowledgeable of this product. After 3 days I received a phone call back from a corporate manager stating they do not offer this product, and they were \" checking into why this is on their website ''. She stated they do have a few customers that make bi-weekly payments, but they no longer offer this service. \n",
      "I don't understand how they can have this active link on their website under their Financial Planning Center tab to mislead customers when all they say is \" I'm sorry, I know you're upset about this '' Sounds like false advertising to me! \n",
      "https : //www.bbt.com/XXXX\n"
     ]
    }
   ],
   "source": [
    "print (\"Complaint 1: \", complaint_1)\n",
    "print (\"\\n\")\n",
    "print (\"Complaint 2: \", complaint_2)\n",
    "print (\"\\n\")\n",
    "print (\"Complaint 3: \", complaint_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents called sents\n",
    "sents = [complaint_1, complaint_2, complaint_3]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.',\n",
       " 'I tried to sign up for a spending monitoring program and Capital One will not let me access my account through them',\n",
       " 'My mortgage is with BB & T Bank, recently I have been investigating ways to pay down my mortgage faster and I came across Biweekly Mortgage Calculator on BB & T \\'s website. It\\'s a nice, easy to use calculator that you plug in your interest rate, mortgage amount, mortgage term, and payment type and it calculates your accelerated bi-weekly payment for you and shows you how much quicker you can pay down your loan. Ours figured out to pay off a 30 year mortgage in 26.4 years ... quite a savings! \\nI called BB & T \\'s customer service number to inquire how I get set up on this payment plan. I was told they do not offer that type of payment plan, but I could send in my payments bi-weekly but it would not be applied until the full amount was received. ( the money would sit in a \" holding account \\'\\' until the full payment amount was collected ). I ended up calling back a few days later thinking the rep I was talking to didn\\'t understand what I wanted to do or was not knowledgeable of this program. I got the SAME ANSWER! \\nI then asked for the corporate BB & T office number where I could speak to someone that was knowledgeable of this product. After 3 days I received a phone call back from a corporate manager stating they do not offer this product, and they were \" checking into why this is on their website \\'\\'. She stated they do have a few customers that make bi-weekly payments, but they no longer offer this service. \\nI don\\'t understand how they can have this active link on their website under their Financial Planning Center tab to mislead customers when all they say is \" I\\'m sorry, I know you\\'re upset about this \\'\\' Sounds like false advertising to me! \\nhttps : //www.bbt.com/XXXX']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(sents)\n",
    "vector = vectorizer.transform(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x214 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 251 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 214)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape #214 Unique words in both sentences combined. 3 documents in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_values = vector.toarray().tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector_values would be the tf-idf score for each of the 214 words. Printing the first 5 elements of this list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out which these words are, just like the count vectorizer, we have the vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'when': 202, 'my': 112, 'loan': 101, 'was': 196, 'switched': 170, 'over': 128, 'to': 183, 'navient': 113, 'never': 114, 'told': 184, 'that': 174, 'had': 78, 'deliquint': 54, 'balance': 19, 'because': 24, 'with': 206, 'xxxx': 209, 'did': 55, 'not': 117, 'going': 76, 'purchase': 140, 'vehicle': 194, 'discovered': 57, 'credit': 49, 'score': 152, 'been': 25, 'dropped': 62, 'from': 73, 'the': 175, 'into': 88, 'have': 79, 'faithful': 66, 'at': 17, 'paying': 131, 'student': 169, 'company': 44, 'delinquency': 53, 'contacted': 46, 'resolve': 148, 'this': 181, 'issue': 91, 'you': 212, 'and': 12, 'kept': 94, 'being': 26, 'just': 93, 'contact': 45, 'bureaus': 30, 'expalin': 65, 'situation': 160, 'maybe': 105, 'they': 179, 'could': 48, 'help': 80, 'me': 106, 'so': 161, 'angry': 13, 'hurried': 84, 'paid': 129, 'off': 120, 'then': 178, 'after': 9, 'tried': 185, 'dispute': 58, 'much': 111, 'trouble': 186, 'bringing': 29, 'back': 18, 'up': 191, 'sign': 158, 'for': 72, 'spending': 166, 'monitoring': 109, 'program': 139, 'capital': 39, 'one': 124, 'will': 205, 'let': 98, 'access': 4, 'account': 5, 'through': 182, 'them': 177, 'mortgage': 110, 'is': 90, 'bb': 21, 'bank': 20, 'recently': 146, 'investigating': 89, 'ways': 197, 'pay': 130, 'down': 61, 'faster': 68, 'came': 37, 'across': 6, 'biweekly': 28, 'calculator': 33, 'on': 123, 'website': 198, 'it': 92, 'nice': 115, 'easy': 63, 'use': 193, 'plug': 137, 'in': 85, 'your': 213, 'interest': 87, 'rate': 143, 'amount': 11, 'term': 173, 'payment': 132, 'type': 187, 'calculates': 32, 'accelerated': 3, 'bi': 27, 'weekly': 199, 'shows': 157, 'how': 82, 'quicker': 141, 'can': 38, 'ours': 126, 'figured': 70, 'out': 127, '30': 1, 'year': 210, '26': 0, 'years': 211, 'quite': 142, 'savings': 150, 'called': 35, 'customer': 50, 'service': 154, 'number': 118, 'inquire': 86, 'get': 75, 'set': 155, 'plan': 135, 'do': 59, 'offer': 121, 'of': 119, 'but': 31, 'send': 153, 'payments': 133, 'would': 207, 'be': 23, 'applied': 15, 'until': 190, 'full': 74, 'received': 145, 'money': 108, 'sit': 159, 'holding': 81, 'collected': 42, 'ended': 64, 'calling': 36, 'few': 69, 'days': 52, 'later': 97, 'thinking': 180, 'rep': 147, 'talking': 172, 'didn': 56, 'understand': 189, 'what': 201, 'wanted': 195, 'or': 125, 'knowledgeable': 96, 'got': 77, 'same': 149, 'answer': 14, 'asked': 16, 'corporate': 47, 'office': 122, 'where': 203, 'speak': 165, 'someone': 162, 'product': 138, 'phone': 134, 'call': 34, 'manager': 104, 'stating': 168, 'were': 200, 'checking': 41, 'why': 204, 'their': 176, 'she': 156, 'stated': 167, 'customers': 51, 'make': 103, 'no': 116, 'longer': 102, 'don': 60, 'active': 7, 'link': 100, 'under': 188, 'financial': 71, 'planning': 136, 'center': 40, 'tab': 171, 'mislead': 107, 'all': 10, 'say': 151, 'sorry': 163, 'know': 95, 're': 144, 'upset': 192, 'about': 2, 'sounds': 164, 'like': 99, 'false': 67, 'advertising': 8, 'https': 83, 'www': 208, 'bbt': 22, 'com': 43}\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_x = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "words = [x[0] for x in sorted_x]\n",
    "d = dict(zip(words,vector_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'26': 0.0, '30': 0.0, 'about': 0.0, 'accelerated': 0.0, 'access': 0.0, 'account': 0.0, 'across': 0.0, 'active': 0.0, 'advertising': 0.0, 'after': 0.05253580411952334, 'all': 0.0, 'amount': 0.0, 'and': 0.20399369240069398, 'angry': 0.06907826902804956, 'answer': 0.0, 'applied': 0.0, 'asked': 0.0, 'at': 0.06907826902804956, 'back': 0.05253580411952334, 'balance': 0.13815653805609912, 'bank': 0.0, 'bb': 0.0, 'bbt': 0.0, 'be': 0.0, 'because': 0.06907826902804956, 'been': 0.10507160823904668, 'being': 0.06907826902804956, 'bi': 0.0, 'biweekly': 0.0, 'bringing': 0.06907826902804956, 'bureaus': 0.13815653805609912, 'but': 0.0, 'calculates': 0.0, 'calculator': 0.0, 'call': 0.0, 'called': 0.0, 'calling': 0.0, 'came': 0.0, 'can': 0.0, 'capital': 0.0, 'center': 0.0, 'checking': 0.0, 'collected': 0.0, 'com': 0.0, 'company': 0.06907826902804956, 'contact': 0.06907826902804956, 'contacted': 0.06907826902804956, 'corporate': 0.0, 'could': 0.05253580411952334, 'credit': 0.27631307611219824, 'customer': 0.0, 'customers': 0.0, 'days': 0.0, 'delinquency': 0.13815653805609912, 'deliquint': 0.06907826902804956, 'did': 0.06907826902804956, 'didn': 0.0, 'discovered': 0.06907826902804956, 'dispute': 0.06907826902804956, 'do': 0.0, 'don': 0.0, 'down': 0.0, 'dropped': 0.06907826902804956, 'easy': 0.0, 'ended': 0.0, 'expalin': 0.06907826902804956, 'faithful': 0.06907826902804956, 'false': 0.0, 'faster': 0.0, 'few': 0.0, 'figured': 0.0, 'financial': 0.0, 'for': 0.0, 'from': 0.05253580411952334, 'full': 0.0, 'get': 0.0, 'going': 0.06907826902804956, 'got': 0.0, 'had': 0.27631307611219824, 'have': 0.10507160823904668, 'help': 0.06907826902804956, 'holding': 0.0, 'how': 0.0, 'https': 0.0, 'hurried': 0.06907826902804956, 'in': 0.0, 'inquire': 0.0, 'interest': 0.0, 'into': 0.05253580411952334, 'investigating': 0.0, 'is': 0.0, 'issue': 0.06907826902804956, 'it': 0.0, 'just': 0.13815653805609912, 'kept': 0.06907826902804956, 'know': 0.0, 'knowledgeable': 0.0, 'later': 0.0, 'let': 0.0, 'like': 0.0, 'link': 0.0, 'loan': 0.10507160823904668, 'longer': 0.0, 'make': 0.0, 'manager': 0.0, 'maybe': 0.06907826902804956, 'me': 0.0407987384801388, 'mislead': 0.0, 'money': 0.0, 'monitoring': 0.0, 'mortgage': 0.0, 'much': 0.05253580411952334, 'my': 0.1631949539205552, 'navient': 0.20723480708414868, 'never': 0.06907826902804956, 'nice': 0.0, 'no': 0.0, 'not': 0.0407987384801388, 'number': 0.0, 'of': 0.0, 'off': 0.05253580411952334, 'offer': 0.0, 'office': 0.0, 'on': 0.0, 'one': 0.0, 'or': 0.0, 'ours': 0.0, 'out': 0.0, 'over': 0.06907826902804956, 'paid': 0.06907826902804956, 'pay': 0.0, 'paying': 0.06907826902804956, 'payment': 0.0, 'payments': 0.0, 'phone': 0.0, 'plan': 0.0, 'planning': 0.0, 'plug': 0.0, 'product': 0.0, 'program': 0.0, 'purchase': 0.06907826902804956, 'quicker': 0.0, 'quite': 0.0, 'rate': 0.0, 're': 0.0, 'received': 0.0, 'recently': 0.0, 'rep': 0.0, 'resolve': 0.06907826902804956, 'same': 0.0, 'savings': 0.0, 'say': 0.0, 'score': 0.13815653805609912, 'send': 0.0, 'service': 0.0, 'set': 0.0, 'she': 0.0, 'shows': 0.0, 'sign': 0.0, 'sit': 0.0, 'situation': 0.06907826902804956, 'so': 0.13815653805609912, 'someone': 0.0, 'sorry': 0.0, 'sounds': 0.0, 'speak': 0.0, 'spending': 0.0, 'stated': 0.0, 'stating': 0.0, 'student': 0.06907826902804956, 'switched': 0.06907826902804956, 'tab': 0.0, 'talking': 0.0, 'term': 0.0, 'that': 0.15760741235857004, 'the': 0.42028643295618673, 'their': 0.0, 'them': 0.0, 'then': 0.05253580411952334, 'they': 0.05253580411952334, 'thinking': 0.0, 'this': 0.05253580411952334, 'through': 0.0, 'to': 0.20399369240069398, 'told': 0.15760741235857004, 'tried': 0.05253580411952334, 'trouble': 0.06907826902804956, 'type': 0.0, 'under': 0.0, 'understand': 0.0, 'until': 0.0, 'up': 0.0407987384801388, 'upset': 0.0, 'use': 0.0, 'vehicle': 0.06907826902804956, 'wanted': 0.0, 'was': 0.2626790205976167, 'ways': 0.0, 'website': 0.0, 'weekly': 0.0, 'were': 0.0, 'what': 0.0, 'when': 0.10507160823904668, 'where': 0.0, 'why': 0.0, 'will': 0.0, 'with': 0.15760741235857004, 'would': 0.0, 'www': 0.0, 'xxxx': 0.15760741235857004, 'year': 0.0, 'years': 0.0, 'you': 0.05253580411952334, 'your': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting this dictionary by value in the descending order to see the ranking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 0.42028643295618673), ('credit', 0.27631307611219824), ('had', 0.27631307611219824), ('was', 0.2626790205976167), ('navient', 0.20723480708414868), ('and', 0.20399369240069398), ('to', 0.20399369240069398), ('my', 0.1631949539205552), ('that', 0.15760741235857004), ('told', 0.15760741235857004), ('with', 0.15760741235857004), ('xxxx', 0.15760741235857004), ('balance', 0.13815653805609912), ('bureaus', 0.13815653805609912), ('delinquency', 0.13815653805609912), ('just', 0.13815653805609912), ('score', 0.13815653805609912), ('so', 0.13815653805609912), ('been', 0.10507160823904668), ('have', 0.10507160823904668), ('loan', 0.10507160823904668), ('when', 0.10507160823904668), ('angry', 0.06907826902804956), ('at', 0.06907826902804956), ('because', 0.06907826902804956), ('being', 0.06907826902804956), ('bringing', 0.06907826902804956), ('company', 0.06907826902804956), ('contact', 0.06907826902804956), ('contacted', 0.06907826902804956), ('deliquint', 0.06907826902804956), ('did', 0.06907826902804956), ('discovered', 0.06907826902804956), ('dispute', 0.06907826902804956), ('dropped', 0.06907826902804956), ('expalin', 0.06907826902804956), ('faithful', 0.06907826902804956), ('going', 0.06907826902804956), ('help', 0.06907826902804956), ('hurried', 0.06907826902804956), ('issue', 0.06907826902804956), ('kept', 0.06907826902804956), ('maybe', 0.06907826902804956), ('never', 0.06907826902804956), ('over', 0.06907826902804956), ('paid', 0.06907826902804956), ('paying', 0.06907826902804956), ('purchase', 0.06907826902804956), ('resolve', 0.06907826902804956), ('situation', 0.06907826902804956), ('student', 0.06907826902804956), ('switched', 0.06907826902804956), ('trouble', 0.06907826902804956), ('vehicle', 0.06907826902804956), ('after', 0.05253580411952334), ('back', 0.05253580411952334), ('could', 0.05253580411952334), ('from', 0.05253580411952334), ('into', 0.05253580411952334), ('much', 0.05253580411952334), ('off', 0.05253580411952334), ('then', 0.05253580411952334), ('they', 0.05253580411952334), ('this', 0.05253580411952334), ('tried', 0.05253580411952334), ('you', 0.05253580411952334), ('me', 0.0407987384801388), ('not', 0.0407987384801388), ('up', 0.0407987384801388), ('26', 0.0), ('30', 0.0), ('about', 0.0), ('accelerated', 0.0), ('access', 0.0), ('account', 0.0), ('across', 0.0), ('active', 0.0), ('advertising', 0.0), ('all', 0.0), ('amount', 0.0), ('answer', 0.0), ('applied', 0.0), ('asked', 0.0), ('bank', 0.0), ('bb', 0.0), ('bbt', 0.0), ('be', 0.0), ('bi', 0.0), ('biweekly', 0.0), ('but', 0.0), ('calculates', 0.0), ('calculator', 0.0), ('call', 0.0), ('called', 0.0), ('calling', 0.0), ('came', 0.0), ('can', 0.0), ('capital', 0.0), ('center', 0.0), ('checking', 0.0), ('collected', 0.0), ('com', 0.0), ('corporate', 0.0), ('customer', 0.0), ('customers', 0.0), ('days', 0.0), ('didn', 0.0), ('do', 0.0), ('don', 0.0), ('down', 0.0), ('easy', 0.0), ('ended', 0.0), ('false', 0.0), ('faster', 0.0), ('few', 0.0), ('figured', 0.0), ('financial', 0.0), ('for', 0.0), ('full', 0.0), ('get', 0.0), ('got', 0.0), ('holding', 0.0), ('how', 0.0), ('https', 0.0), ('in', 0.0), ('inquire', 0.0), ('interest', 0.0), ('investigating', 0.0), ('is', 0.0), ('it', 0.0), ('know', 0.0), ('knowledgeable', 0.0), ('later', 0.0), ('let', 0.0), ('like', 0.0), ('link', 0.0), ('longer', 0.0), ('make', 0.0), ('manager', 0.0), ('mislead', 0.0), ('money', 0.0), ('monitoring', 0.0), ('mortgage', 0.0), ('nice', 0.0), ('no', 0.0), ('number', 0.0), ('of', 0.0), ('offer', 0.0), ('office', 0.0), ('on', 0.0), ('one', 0.0), ('or', 0.0), ('ours', 0.0), ('out', 0.0), ('pay', 0.0), ('payment', 0.0), ('payments', 0.0), ('phone', 0.0), ('plan', 0.0), ('planning', 0.0), ('plug', 0.0), ('product', 0.0), ('program', 0.0), ('quicker', 0.0), ('quite', 0.0), ('rate', 0.0), ('re', 0.0), ('received', 0.0), ('recently', 0.0), ('rep', 0.0), ('same', 0.0), ('savings', 0.0), ('say', 0.0), ('send', 0.0), ('service', 0.0), ('set', 0.0), ('she', 0.0), ('shows', 0.0), ('sign', 0.0), ('sit', 0.0), ('someone', 0.0), ('sorry', 0.0), ('sounds', 0.0), ('speak', 0.0), ('spending', 0.0), ('stated', 0.0), ('stating', 0.0), ('tab', 0.0), ('talking', 0.0), ('term', 0.0), ('their', 0.0), ('them', 0.0), ('thinking', 0.0), ('through', 0.0), ('type', 0.0), ('under', 0.0), ('understand', 0.0), ('until', 0.0), ('upset', 0.0), ('use', 0.0), ('wanted', 0.0), ('ways', 0.0), ('website', 0.0), ('weekly', 0.0), ('were', 0.0), ('what', 0.0), ('where', 0.0), ('why', 0.0), ('will', 0.0), ('would', 0.0), ('www', 0.0), ('year', 0.0), ('years', 0.0), ('your', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "print (sorted(d.items(), key=operator.itemgetter(1), reverse = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model learns to give lesser importance to words like is,it,in etc;. Unfortunately, it also gives a low importance to important words like financial, mortgage and a fairly high importance to unwanted words like the, was. It does give higher importance to words such as credit. And that is because TF-DF works better with larger corpuses. Just like a machine learning model, the larger the data, the better the model.  With a larger corpus, these issues would be resolved when a lot more documents would have words like financial but not the.\n",
    "\n",
    "Rerunning this fo about 100 documents, we see that the ranking is completely different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=[]\n",
    "for x in range(100):\n",
    "    sents.append(df[\"X\"].iloc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('navient', 0.35431369455512246), ('the', 0.24041090745878946), ('delinquency', 0.23620912970341496), ('had', 0.20963581996093744), ('told', 0.19801036084460227), ('bureaus', 0.19189624902422267), ('was', 0.18441012909485335), ('score', 0.1637072393027124), ('credit', 0.15907030235158917), ('just', 0.15203704157649714), ('balance', 0.14549112699503114), ('to', 0.14437952896171652), ('and', 0.14013869297167467), ('loan', 0.1344398602659683), ('angry', 0.12870728663065903), ('deliquint', 0.12870728663065903), ('expalin', 0.12870728663065903), ('faithful', 0.12870728663065903), ('hurried', 0.12870728663065903), ('switched', 0.12870728663065903)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents called sents\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(sents)\n",
    "vector = vectorizer.transform(sents)\n",
    "vector.shape \n",
    "vector_values = vector.toarray().tolist()[0]\n",
    "import operator\n",
    "sorted_x = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "words = [x[0] for x in sorted_x]\n",
    "d = dict(zip(words,vector_values))\n",
    "print ((sorted(d.items(), key=operator.itemgetter(1), reverse = True))[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"the\" has moved down from 0.42 to 0.24. 'Navient' has increased from 0.20 to 0.35. So has 'bureaus' from 0.13 to 0.19. As we include more and more sentences, the words whch have appeared more and more frequently across all the documents, such as \"the\" are moving down in value, and words like bureau and navient, which have appeared far lesser number of times have started increasing. Which reiterates the point we had. TF-DF works better with larger corpuses. Just like a machine learning model, the larger the data, the better the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our initial Logistic Regression model using a tf-idf and see if there is a difference in the accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3764705882352941\n"
     ]
    }
   ],
   "source": [
    "all_text = df[\"X\"]\n",
    "all_text = pd.DataFrame(all_text)\n",
    "all_text.columns = [\"Text\"]\n",
    "all_text[\"Text\"] = all_text['Text'].str.lower()\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf.fit(all_text[\"Text\"])\n",
    "vector = tfidf.transform(all_text[\"Text\"])\n",
    "vector_values_array = vector.toarray()\n",
    "labels = pd.DataFrame(df[\"y\"])\n",
    "labels.columns = [\"labels\"]\n",
    "labels[\"labels\"] = le.fit_transform(labels[\"labels\"])\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "X = vector_values_array\n",
    "y = labels[\"labels\"]\n",
    "X_train,X_test,y_train,y_test = tts(X,y,test_size=0.4,random_state=42)\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train,y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print (accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that the overall accuracy of the model is **low** compared to the initial CV model. A 10% reduction. As far we have seen, do you think this will increase if we add more data to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"Consumer_complaints.pkl\")\n",
    "print (\"reading\")\n",
    "df_copy = df.tail(2000).copy()\n",
    "df = df_copy[[\"Consumer complaint narrative\", \"Product\"]] #keeping the relevant columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1998, 2)\n",
      "Building model\n",
      "0.5783333333333334\n"
     ]
    }
   ],
   "source": [
    "df.columns = [\"X\",\"y\"]\n",
    "df = df.dropna()\n",
    "df = df.iloc[:2000]\n",
    "print (df.shape)\n",
    "print (\"Building model\")\n",
    "all_text = df[\"X\"]\n",
    "all_text = pd.DataFrame(all_text)\n",
    "all_text.columns = [\"Text\"]\n",
    "all_text[\"Text\"] = all_text['Text'].str.lower()\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf.fit(all_text[\"Text\"])\n",
    "vector = tfidf.transform(all_text[\"Text\"])\n",
    "vector_values_array = vector.toarray()\n",
    "labels = pd.DataFrame(df[\"y\"])\n",
    "labels.columns = [\"labels\"]\n",
    "labels[\"labels\"] = le.fit_transform(labels[\"labels\"])\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "X = vector_values_array\n",
    "y = labels[\"labels\"]\n",
    "X_train,X_test,y_train,y_test = tts(X,y,test_size=0.3,random_state=42)\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train,y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print (accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 43% at 359 rows, to 59% at 2000 rows, we are starting to see how TF-IDF works better with larger\n",
    "data sets. The last value for the word \"the\" was around 0.27. Just out of curiosity, let's see the value of the word \"the\" now at 2000 docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('xxxx', 0.37152555551310695), ('sell', 0.35476801553845877), ('mothers', 0.3261854926117282), ('wells', 0.20454943984068547), ('slowing', 0.18940866547356533), ('finalize', 0.1796962427013482), ('extract', 0.1674600371150138), ('feet', 0.1630927463058641), ('dragging', 0.1594002545398214), ('sought', 0.1594002545398214), ('to', 0.1576799906365181), ('mom', 0.15338032353364697), ('nc', 0.15338032353364697), ('deceased', 0.15085654071952967), ('help', 0.1475045824229167), ('home', 0.14562561360436718), ('my', 0.14059112518073988), ('totally', 0.13548171271131101), ('efforts', 0.13308433537212017), ('xxxxxxxxxxxx', 0.13308433537212017)]\n"
     ]
    }
   ],
   "source": [
    "### from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents called sents\n",
    "# create the transform\n",
    "sents=[]\n",
    "for x in range(df.shape[0]):\n",
    "    sents.append(df[\"X\"].iloc[x])\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(sents)\n",
    "vector = vectorizer.transform(sents)\n",
    "vector.shape \n",
    "vector_values = vector.toarray().tolist()[0]\n",
    "import operator\n",
    "sorted_x = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "words = [x[0] for x in sorted_x]\n",
    "d = dict(zip(words,vector_values))\n",
    "print ((sorted(d.items(), key=operator.itemgetter(1), reverse = True))[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"the\" is at 0.21 at 2000 documents. \"And\" has been pushed down, expalin has come up, deliquent has increased etc; a few words have been pushed down as well, based on their importance **across all documents** not just each document alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: The Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifers are based on the Bayes' theorem. A pure classification algorithm, it predicts the various categories or classes of the target based on the fundamental premise that the features responsible are independent of each other. These features are assumed to independently contribute to the the probability to the target variable belonging to a certain class. \n",
    "\n",
    "Let us try running a Naive Bayes classifier using the TF-IDF method, exactly the same way we ran a log-reg model in the earlier section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading\n",
      "(2000, 2)\n",
      "Building model\n",
      "0.415\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"Consumer_complaints.pkl\")\n",
    "print (\"reading\")\n",
    "df_copy = df.copy()\n",
    "df = df[[\"Consumer complaint narrative\", \"Product\"]] #keeping the relevant columns\n",
    "df.shape\n",
    "df.columns = [\"X\",\"y\"]\n",
    "df = df.dropna()\n",
    "df = df.iloc[:2000]\n",
    "print (df.shape)\n",
    "print (\"Building model\")\n",
    "all_text = df[\"X\"]\n",
    "all_text = pd.DataFrame(all_text)\n",
    "all_text.columns = [\"Text\"]\n",
    "all_text[\"Text\"] = all_text['Text'].str.lower()\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf.fit(all_text[\"Text\"])\n",
    "vector = tfidf.transform(all_text[\"Text\"])\n",
    "vector_values_array = vector.toarray()\n",
    "labels = pd.DataFrame(df[\"y\"])\n",
    "labels.columns = [\"labels\"]\n",
    "labels[\"labels\"] = le.fit_transform(labels[\"labels\"])\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "X = vector_values_array\n",
    "y = labels[\"labels\"]\n",
    "X_train,X_test,y_train,y_test = tts(X,y,test_size=0.3,random_state=42)\n",
    "# log_reg = LogisticRegression(random_state=42)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train,y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "print (accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the fundamental reasons the model isn't giving an accuracy can be deduced from the classification report, where we see that the recall of multiple categories is 0 and that means that the data isn't balanced well enough to reflect an equal weightage. Since a classfication algorithm, tends to predict the majority class unless, the output categories are more or less equally balanced, the error in preeicting the majority class will increase as it classfies more and more data, and classifies tg=hem worng, hence reducing the overall accuracy. \n",
    "\n",
    "A recap on the Classification report of the NB classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading\n",
      "(2000, 2)\n",
      "Building model\n",
      "0.415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.00      0.00      0.00        21\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00        22\n",
      "           4       0.00      0.00      0.00        42\n",
      "           5       0.00      0.00      0.00        50\n",
      "           6       0.36      1.00      0.53       190\n",
      "           7       0.88      0.41      0.56       140\n",
      "           8       0.00      0.00      0.00         9\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       1.00      0.02      0.03        58\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.00      0.00      0.00         6\n",
      "          13       0.00      0.00      0.00         5\n",
      "          14       0.00      0.00      0.00        21\n",
      "          15       0.00      0.00      0.00         8\n",
      "\n",
      "   micro avg       0.41      0.41      0.41       600\n",
      "   macro avg       0.14      0.09      0.07       600\n",
      "weighted avg       0.41      0.41      0.30       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"Consumer_complaints.pkl\")\n",
    "print (\"reading\")\n",
    "df_copy = df.copy()\n",
    "df = df[[\"Consumer complaint narrative\", \"Product\"]] #keeping the relevant columns\n",
    "df.shape\n",
    "df.columns = [\"X\",\"y\"]\n",
    "df = df.dropna()\n",
    "df = df.iloc[:2000]\n",
    "print (df.shape)\n",
    "print (\"Building model\")\n",
    "all_text = df[\"X\"]\n",
    "all_text = pd.DataFrame(all_text)\n",
    "all_text.columns = [\"Text\"]\n",
    "all_text[\"Text\"] = all_text['Text'].str.lower()\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf.fit(all_text[\"Text\"])\n",
    "vector = tfidf.transform(all_text[\"Text\"])\n",
    "vector_values_array = vector.toarray()\n",
    "labels = pd.DataFrame(df[\"y\"])\n",
    "labels.columns = [\"labels\"]\n",
    "labels[\"labels\"] = le.fit_transform(labels[\"labels\"])\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "X = vector_values_array\n",
    "y = labels[\"labels\"]\n",
    "X_train,X_test,y_train,y_test = tts(X,y,test_size=0.3,random_state=42)\n",
    "# log_reg = LogisticRegression(random_state=42)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train,y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "print (accuracy_score(y_test,y_pred))\n",
    "print (classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     669\n",
       "7     469\n",
       "10    206\n",
       "4     128\n",
       "5     123\n",
       "14     96\n",
       "1      80\n",
       "3      64\n",
       "0      37\n",
       "2      33\n",
       "15     31\n",
       "8      27\n",
       "12     21\n",
       "9       7\n",
       "13      5\n",
       "11      4\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As predicted, the dataset is heavily imbalanced. With Category 6 and 7 being over represented , while all else have a less than 10% weightage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might have to oversample the under-represented categories. We will use the Random Over sampler to do this from the package IMBLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"X\",\"y\"]\n",
    "df = df.dropna()\n",
    "df = df.iloc[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading https://files.pythonhosted.org/packages/81/a7/4179e6ebfd654bd0eac0b9c06125b8b4c96a9d0a8ff9e9507eb2a26d2d7e/imblearn-0.0-py2.py3-none-any.whl\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/4c/7557e1c2e791bd43878f8c82065bddc5798252084f26ef44527c02262af1/imbalanced_learn-0.4.3-py3-none-any.whl (166kB)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\ashwani.saxena\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\users\\ashwani.saxena\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.13.3 in c:\\users\\ashwani.saxena\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.4.3 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"X\"]\n",
    "y = df[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.fit(X[\"X\"])\n",
    "vector = tfidf.transform(X[\"X\"])\n",
    "vector_values = vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_values = pd.DataFrame(vector_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(y)\n",
    "labels.columns = [\"labels\"]\n",
    "labels[\"labels\"] = le.fit_transform(labels[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "X_ros, y_ros = ros.fit_sample(vector_values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ros = pd.Series(y_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9430261519302615"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = tts(X_ros,y_ros,test_size = 0.3, random_state = 0)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train,y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We managed a 94% accuracy with multinominal Naive Bayes, TF-IDF and oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Linear SVC as our last algorithm, to test the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9713574097135741"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "X_train,X_test,y_train,y_test = tts(X_ros,y_ros,test_size = 0.3, random_state = 0)\n",
    "svc = SVC(kernel=\"linear\")\n",
    "svc.fit(X_train,y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
